"""
[IMPORTANT NOTE / Ù…Ù„Ø§Ø­Ø¸Ø© Ù‡Ø§Ù…Ø©]
--------------------------------------------------
English: This script is specifically designed and optimized to run in the GOOGLE COLAB environment.
- It is configured to automatically download models and training files directly from GitHub.
- Copy-pasting this code to other environments (local IDEs) may require adjustments 
  to file paths and library configurations.

Arabic: Google Colab Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø®ØµØµ ÙˆÙ…Ø¬Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¯Ø§Ø®Ù„ Ø¨ÙŠØ¦Ø© 
- GitHub Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠ ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒÙˆØ¯ Ù„ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† 
- Ù†Ø³Ø® Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙˆØªØ´ØºÙŠÙ„Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø£Ùˆ Ø¨ÙŠØ¦Ø§Øª Ø£Ø®Ø±Ù‰ Ù‚Ø¯ ÙŠØªØ·Ù„Ø¨ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ÙÙŠ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª.
--------------------------------------------------
Created by: Yahya Zuher
Project: AI-Liver-Diseases-Diagnosis-System

        - Target Output: 0.0 - 1.0 Presentage or int(0,1)

"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import joblib
import os
import sys

# --- Configuration (GitHub Integration) ---
RAW_DATA_URL = 'https://raw.githubusercontent.com/yahyazuher/AI-Liver-Diseases-Diagnosis-System/main/data/processed/hepatitisC_status.csv'
MODEL_FILENAME = 'hepatitisC_status_model.pkl'

def get_live_dataset():
    """Downloads the dataset from GitHub."""
    try:
        print(f"â¬‡ï¸ Downloading dataset from GitHub...")
        df = pd.read_csv(RAW_DATA_URL)
        print(f"âœ… Successfully loaded {len(df)} records.")
        return df
    except Exception as e:
        sys.exit(f"âŒ Critical Error: Could not fetch data. {e}")

def add_medical_features(df):
    """Adds ALBI and APRI features for high-precision mortality prediction."""
    df_eng = df.copy()

    # 1. APRI Score
    df_eng['APRI'] = ((df_eng['SGOT'] / 40.0) / (df_eng['Platelets'] + 0.1)) * 100

    # 2. ALBI Score (Log-based liver function assessment)
    df_eng['ALBI_Score'] = (np.log10(df_eng['Bilirubin'].clip(lower=0.1) * 17.1) * 0.66) + \
                            (df_eng['Albumin'] * 10 * -0.085)

    # 3. Bilirubin to Albumin Ratio
    df_eng['Bili_Alb_Ratio'] = df_eng['Bilirubin'] / (df_eng['Albumin'] + 0.1)

    return df_eng

def run_pipeline():
    print("ğŸš€ Starting Automated Training Pipeline...")

    # 1. Get Data from GitHub
    df = get_live_dataset()

    # 2. Add Engineered Features
    df = add_medical_features(df)

    # 3. Define Features & Target
    # We exclude 'Status' (Target) and 'Stage' (due to its low accuracy)
    X = df.drop(columns=['Status', 'Stage'], errors='ignore')
    y = df['Status'].astype(int)

    # 4. Preprocessing Layers
    numeric_features = X.select_dtypes(include=['number']).columns
    categorical_features = X.select_dtypes(include=['object']).columns

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

    # 5. XGBoost Model Configuration (Tuned for 125/187 ratio)
    model = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', xgb.XGBClassifier(
            n_estimators=250,
            learning_rate=0.03,
            max_depth=4,
            scale_pos_weight=1.5, # Balance for your specific dataset
            subsample=0.8,
            colsample_bytree=0.8,
            objective='binary:logistic',
            eval_metric='logloss',
            random_state=42
        ))
    ])

    # 6. Stratified Train-Test Split (80/20)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 7. Model Training
    print("âš™ï¸ Training model on remote data...")
    model.fit(X_train, y_train)

    # 8. Evaluation
    y_pred = model.predict(X_test)
    print("\nğŸ“ˆ Final Validation Results:")
    print(f"ğŸ¯ Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
    print("-" * 30)
    print(classification_report(y_test, y_pred))

    # 9. Global Save
    print(f"ğŸ’¾ Saving model locally: {MODEL_FILENAME}")
    model.fit(X, y) # Retrain on full dataset
    joblib.dump(model, MODEL_FILENAME)
    print("âœ… Done! The model is now updated and ready for deployment.")

if __name__ == "__main__":
    run_pipeline()
